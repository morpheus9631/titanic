{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score, make_scorer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from xgboost import XGBClassifier, plot_importance as plot_importance_xgb\n",
    "from lightgbm import LGBMClassifier, plot_importance as plot_importance_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "\n",
    "DataPath = r'D:\\GitWork\\titanic\\data'\n",
    "TranFile = 'train.csv'\n",
    "TestFile = 'test.csv'\n",
    "OutFile  = 'gender_submission.csv'\n",
    "\n",
    "absp_train = join(DataPath, TranFile)\n",
    "df = pd.read_csv(absp_train)\n",
    "print(df.info())\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\opt\\miniconda3\\envs\\py37\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>AgeCat</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>SexCat</th>\n",
       "      <th>Title</th>\n",
       "      <th>Is_Married</th>\n",
       "      <th>Ticket_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>mature</td>\n",
       "      <td>small</td>\n",
       "      <td>no</td>\n",
       "      <td>maturemale</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>mature</td>\n",
       "      <td>small</td>\n",
       "      <td>no</td>\n",
       "      <td>maturefemale</td>\n",
       "      <td>Miss/Mrs/Ms</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>mature</td>\n",
       "      <td>small</td>\n",
       "      <td>yes</td>\n",
       "      <td>maturefemale</td>\n",
       "      <td>Miss/Mrs/Ms</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>mature</td>\n",
       "      <td>small</td>\n",
       "      <td>no</td>\n",
       "      <td>maturefemale</td>\n",
       "      <td>Miss/Mrs/Ms</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>mature</td>\n",
       "      <td>small</td>\n",
       "      <td>yes</td>\n",
       "      <td>maturemale</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td></td>\n",
       "      <td>small</td>\n",
       "      <td>yes</td>\n",
       "      <td>maturemale</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "      <td>mature</td>\n",
       "      <td>small</td>\n",
       "      <td>yes</td>\n",
       "      <td>seniormale</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>young</td>\n",
       "      <td>medium</td>\n",
       "      <td>no</td>\n",
       "      <td>maturemale</td>\n",
       "      <td>Master</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>mature</td>\n",
       "      <td>small</td>\n",
       "      <td>no</td>\n",
       "      <td>maturefemale</td>\n",
       "      <td>Miss/Mrs/Ms</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>young</td>\n",
       "      <td>small</td>\n",
       "      <td>no</td>\n",
       "      <td>maturefemale</td>\n",
       "      <td>Miss/Mrs/Ms</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  AgeCat FamilySize IsAlone  \\\n",
       "0      0         A/5 21171   7.2500   NaN        S  mature      small      no   \n",
       "1      0          PC 17599  71.2833   C85        C  mature      small      no   \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  mature      small     yes   \n",
       "3      0            113803  53.1000  C123        S  mature      small      no   \n",
       "4      0            373450   8.0500   NaN        S  mature      small     yes   \n",
       "5      0            330877   8.4583   NaN        Q              small     yes   \n",
       "6      0             17463  51.8625   E46        S  mature      small     yes   \n",
       "7      1            349909  21.0750   NaN        S   young     medium      no   \n",
       "8      2            347742  11.1333   NaN        S  mature      small      no   \n",
       "9      0            237736  30.0708   NaN        C   young      small      no   \n",
       "\n",
       "         SexCat        Title  Is_Married  Ticket_Frequency  \n",
       "0    maturemale           Mr           0                 1  \n",
       "1  maturefemale  Miss/Mrs/Ms           1                 1  \n",
       "2  maturefemale  Miss/Mrs/Ms           0                 1  \n",
       "3  maturefemale  Miss/Mrs/Ms           1                 2  \n",
       "4    maturemale           Mr           0                 1  \n",
       "5    maturemale           Mr           0                 1  \n",
       "6    seniormale           Mr           0                 1  \n",
       "7    maturemale       Master           0                 4  \n",
       "8  maturefemale  Miss/Mrs/Ms           1                 3  \n",
       "9  maturefemale  Miss/Mrs/Ms           1                 2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a categorical variable for Ages\n",
    "df['AgeCat'] = ''\n",
    "df['AgeCat'].loc[(df['Age'] < 18)] = 'young'\n",
    "df['AgeCat'].loc[(df['Age'] >= 18) & (df['Age'] < 56)] = 'mature'\n",
    "df['AgeCat'].loc[(df['Age'] >= 56)] = 'senior'\n",
    "\n",
    "\n",
    "# Creating a categorical variable for Family Sizes\n",
    "df['FamilySize'] = ''\n",
    "df['FamilySize'].loc[(df['SibSp'] <= 2)] = 'small'\n",
    "df['FamilySize'].loc[(df['SibSp'] > 2) & (df['SibSp'] <= 5 )] = 'medium'\n",
    "df['FamilySize'].loc[(df['SibSp'] > 5)] = 'large'\n",
    "\n",
    "\n",
    "# Creating a categorical variable to tell if the passenger is alone\n",
    "df['IsAlone'] = ''\n",
    "df['IsAlone'].loc[((df['SibSp'] + df['Parch']) > 0)] = 'no'\n",
    "df['IsAlone'].loc[((df['SibSp'] + df['Parch']) == 0)] = 'yes'\n",
    "\n",
    "\n",
    "# Creating a categorical variable to tell if the passenger is a Young/Mature/Senior male or a Young/Mature/Senior female\n",
    "df['SexCat'] = ''\n",
    "df['SexCat'].loc[(df['Sex'] == 'male') & (df['Age'] <= 21)] = 'youngmale'\n",
    "df['SexCat'].loc[(df['Sex'] == 'male') & ((df['Age'] > 21) & (df['Age']) < 50)] = 'maturemale'\n",
    "df['SexCat'].loc[(df['Sex'] == 'male') & (df['Age'] > 50)] = 'seniormale'\n",
    "df['SexCat'].loc[(df['Sex'] == 'female') & (df['Age'] <= 21)] = 'youngfemale'\n",
    "df['SexCat'].loc[(df['Sex'] == 'female') & ((df['Age'] > 21) & (df['Age']) < 50)] = 'maturefemale'\n",
    "df['SexCat'].loc[(df['Sex'] == 'female') & (df['Age'] > 50)] = 'seniorfemale'\n",
    "\n",
    "# Creating a categorical variable for the passenger's title\n",
    "# Title is created by extracting the prefix before \"Name\" feature\n",
    "# This title needs to be a feature because all female titles are grouped with each other\n",
    "# Also, creating a column to tell if the passenger is married or not\n",
    "# \"Is_Married\" is a binary feature based on the Mrs title. Mrs title has the highest survival rate among other female titles\n",
    "df['Title'] = df['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n",
    "df['Is_Married'] = 0\n",
    "df['Is_Married'].loc[df['Title'] == 'Mrs'] = 1\n",
    "df['Title'] = df['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\n",
    "df['Title'] = df['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')\n",
    "\n",
    "\n",
    "# Creating \"Ticket Frequency\" Feature\n",
    "# There are too many unique Ticket values to analyze, so grouping them up by their frequencies makes things easier\n",
    "df['Ticket_Frequency'] = df.groupby('Ticket')['Ticket'].transform('count')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns:\n",
      " ['Sex', 'Embarked', 'AgeCat', 'FamilySize', 'IsAlone', 'SexCat', 'Title']\n",
      "\n",
      "Numeric columns:\n",
      " ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Is_Married', 'Ticket_Frequency']\n"
     ]
    }
   ],
   "source": [
    "def get_feature_names(df):\n",
    "    # Splitting the target\n",
    "    target = df['Survived']\n",
    "\n",
    "    # Dropping unused columns from the feature set\n",
    "    df.drop(['PassengerId', 'Survived', 'Ticket', 'Name', 'Cabin'], axis=1, inplace=True)\n",
    "\n",
    "    # Splitting categorical and numerical column dataframes\n",
    "    categorical_df = df.select_dtypes(include=['object'])\n",
    "    numeric_df = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "    # And then, storing the names of categorical and numerical columns.\n",
    "    categorical_columns = list(categorical_df.columns)\n",
    "    numeric_columns = list(numeric_df.columns)\n",
    "    \n",
    "    print(\"Categorical columns:\\n\", categorical_columns)\n",
    "    print(\"\\nNumeric columns:\\n\", numeric_columns)\n",
    "\n",
    "    return target, categorical_columns, numeric_columns\n",
    "\n",
    "target, categorical_columns, numeric_columns = get_feature_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing Data\n",
    "\n",
    "# You can call any of the functions below, if you wish, inside the \"defineBestModelPipeline()\" function\n",
    "\n",
    "def balancingClassesRus(x_train, y_train):\n",
    "    \n",
    "    # Using RandomUnderSampler to balance our training data points\n",
    "    rus = RandomUnderSampler(random_state=7)\n",
    "    features_balanced, target_balanced = rus.fit_resample(x_train, y_train)\n",
    "    \n",
    "    print(\"Count for each class value after RandomUnderSampler:\", collections.Counter(target_balanced))\n",
    "    \n",
    "    return features_balanced, target_balanced\n",
    "\n",
    "\n",
    "def balancingClassesSmoteenn(x_train, y_train):\n",
    "    \n",
    "    # Using SMOTEEN to balance our training data points\n",
    "    smn = SMOTEENN(random_state=7)\n",
    "    features_balanced, target_balanced = smn.fit_resample(x_train, y_train)\n",
    "    \n",
    "    print(\"Count for each class value after SMOTEEN:\", collections.Counter(target_balanced))\n",
    "    \n",
    "    return features_balanced, target_balanced\n",
    "\n",
    "\n",
    "def balancingClassesSmote(x_train, y_train):\n",
    "\n",
    "    # Using SMOTE to to balance our training data points\n",
    "    sm = SMOTE(random_state=7)\n",
    "    features_balanced, target_balanced = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "    print(\"Count for each class value after SMOTE:\", collections.Counter(target_balanced))\n",
    "\n",
    "    return features_balanced, target_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function responsible for checking our model's performance on the test data\n",
    "def testSetResultsClassifier(classifier, x_test, y_test):\n",
    "    predictions = classifier.predict(x_test)\n",
    "    \n",
    "    results = []\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    results.append(f1)\n",
    "    results.append(precision)\n",
    "    results.append(recall)\n",
    "    results.append(roc_auc)\n",
    "    results.append(accuracy)\n",
    "    \n",
    "    print(\"\\n\\n#---------------- Test set results (Best Classifier) ----------------#\\n\")\n",
    "    print(\"F1 score, Precision, Recall, ROC_AUC score, Accuracy:\")\n",
    "    print(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we are going to create our Pipeline, fitting several different data preprocessing, feature selection \n",
    "# and modeling techniques inside a RandomSearchCV, to check which group of techniques has better performance.\n",
    "\n",
    "# Building a Pipeline inside RandomSearchCV, responsible for finding the best model and it's parameters\n",
    "def defineBestModelPipeline(df, target, categorical_columns, numeric_columns):\n",
    "    \n",
    "    # Splitting original data into Train and Test BEFORE applying transformations\n",
    "    # Later in RandomSearchCV, x_train will be splitted into train/val sets\n",
    "    # The transformations are going to be fitted specifically on the train set,\n",
    "    # and then applied to both train/test sets. This way, information leakage is avoided!\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.10, random_state=42)\n",
    "    y_train = y_train.to_numpy() # Transforming training targets into numpy arrays\n",
    "    y_test = y_test.to_numpy() # Transforming test targets into numpy arrays\n",
    "    \n",
    "    \n",
    "    # # If desired, we can balance training classes using one of the functions below\n",
    "    # # Obtaining balanced data for modeling using Random Under Sampling\n",
    "    #x_train, y_train = balancingClassesRus(x_train, y_train)\n",
    "\n",
    "    # # Obtaining balanced data for modeling using SMOTEENN\n",
    "    #x_train, y_train = balancingClassesSmoteenn(x_train, y_train)\n",
    "\n",
    "    # # Obtaining balanced data for modeling using SMOTE\n",
    "    #x_train, y_train = balancingClassesSmote(x_train, y_train)\n",
    "    \n",
    "    \n",
    "    # 1st -> Numeric Transformers\n",
    "    # Here, we are creating different several different data transformation pipelines \n",
    "    # to be applied in our numeric features\n",
    "    numeric_transformer_1 = Pipeline(steps=[('imp', IterativeImputer(max_iter=30, random_state=42)),\n",
    "                                            ('scaler', MinMaxScaler())])\n",
    "    \n",
    "    numeric_transformer_2 = Pipeline(steps=[('imp', IterativeImputer(max_iter=20, random_state=42)),\n",
    "                                            ('scaler', StandardScaler())])\n",
    "    \n",
    "    numeric_transformer_3 = Pipeline(steps=[('imp', SimpleImputer(strategy='mean')),\n",
    "                                            ('scaler', MinMaxScaler())])\n",
    "    \n",
    "    numeric_transformer_4 = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),\n",
    "                                            ('scaler', StandardScaler())])\n",
    "    \n",
    "    \n",
    "    # 2nd -> Categorical Transformer\n",
    "    # Despite my option of not doing it, you can also choose to create different \n",
    "    # data transformation pipelines for your categorical features.\n",
    "    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),\n",
    "                                              ('onehot', OneHotEncoder(use_cat_names=True))])\n",
    "    \n",
    "    \n",
    "    # 3rd -> Combining both numerical and categorical pipelines\n",
    "    # Here, we are creating different ColumnTransformers, each one with a different numerical transformation\n",
    "    data_transformations_1 = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer_1, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ])\n",
    "    \n",
    "    data_transformations_2 = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer_2, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ])\n",
    "    \n",
    "    data_transformations_3 = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer_3, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ])\n",
    "    \n",
    "    data_transformations_4 = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer_4, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # And finally, we are going to apply these different data transformations to RandomSearchCV,\n",
    "    # trying to find the best imputing strategy, the best feature engineering strategy\n",
    "    # and the best model with it's respective parameters.\n",
    "    # Below, we just need to initialize a Pipeline object with any transformations we want, on each of the steps.\n",
    "    pipe = Pipeline(steps=[('data_transformations', data_transformations_1), # Initializing data transformation step by choosing any of the above\n",
    "                           ('feature_eng', PCA()), # Initializing feature engineering step by choosing any desired method\n",
    "                           ('clf', SVC())]) # Initializing modeling step of the pipeline with any model object\n",
    "                           #memory='cache_folder') -> Used to optimize memory when needed\n",
    "    \n",
    "    \n",
    "    # Now, we define the grid of parameters that RandomSearchCV will use. It will randomly chose\n",
    "    # options for each step inside the dictionaries ('data transformations', 'feature_eng', 'clf'\n",
    "    # and 'clf parameters'). In the end of it's iterations, RandomSearchCV will return the best options.\n",
    "    params_grid = [{\n",
    "        'data_transformations': [\n",
    "            data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4\n",
    "        ],\n",
    "        'feature_eng': [\n",
    "            None, \n",
    "            PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "            PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)\n",
    "        ],\n",
    "        'clf': [KNeighborsClassifier()],\n",
    "        'clf__n_neighbors': stats.randint(1, 50),\n",
    "        'clf__metric': ['minkowski', 'euclidean']\n",
    "    },{\n",
    "        'data_transformations': [\n",
    "            data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4\n",
    "        ],\n",
    "        'feature_eng': [\n",
    "            None, \n",
    "            PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "            PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)\n",
    "        ],\n",
    "        'clf': [LogisticRegression()],\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': stats.uniform(0.01, 10)\n",
    "    }, {\n",
    "        'data_transformations': [\n",
    "            data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4\n",
    "        ],\n",
    "        'feature_eng': [\n",
    "            None, \n",
    "            PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "            PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)\n",
    "        ],\n",
    "        'clf': [SVC()],\n",
    "        'clf__C': stats.uniform(0.01, 1),\n",
    "        'clf__gamma': stats.uniform(0.01, 1)\n",
    "    }, {\n",
    "        'data_transformations': [\n",
    "            data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4\n",
    "        ],\n",
    "        'feature_eng': [\n",
    "            None, \n",
    "            PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "            PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)\n",
    "        ],\n",
    "        'clf': [DecisionTreeClassifier()],\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__max_features': [None, \"auto\", \"log2\"],\n",
    "        'clf__max_depth': [None, stats.randint(1, 5)]\n",
    "    }, {\n",
    "        'data_transformations': [\n",
    "            data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4\n",
    "        ],\n",
    "        'feature_eng': [\n",
    "            None, \n",
    "            PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "            PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)\n",
    "        ],\n",
    "        'clf': [RandomForestClassifier()],\n",
    "        'clf__n_estimators': stats.randint(10, 175),\n",
    "        'clf__max_features': [None, \"auto\", \"log2\"],\n",
    "        'clf__max_depth': [None, stats.randint(1, 5)],\n",
    "        'clf__random_state': stats.randint(1, 49)\n",
    "    }, {\n",
    "        'data_transformations': [\n",
    "            data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4\n",
    "        ],\n",
    "        'feature_eng': [\n",
    "            None, \n",
    "            PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "            PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)\n",
    "        ],\n",
    "        'clf': [ExtraTreesClassifier()],\n",
    "        'clf__n_estimators': stats.randint(10, 150),\n",
    "        'clf__max_features': [None, \"auto\", \"log2\"],\n",
    "        'clf__max_depth': [None, stats.randint(1, 6)]\n",
    "    }, {\n",
    "        'data_transformations': [\n",
    "            data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4\n",
    "        ],\n",
    "        'feature_eng': [\n",
    "            None, \n",
    "            PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "            PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)\n",
    "        ],\n",
    "        'clf': [GradientBoostingClassifier()],\n",
    "        'clf__n_estimators': stats.randint(10, 100),\n",
    "        'clf__learning_rate': stats.uniform(0.01, 0.7),\n",
    "        'clf__max_depth': [None, stats.randint(1, 6)]\n",
    "    }, {\n",
    "        'data_transformations': [\n",
    "            data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4\n",
    "        ],\n",
    "        'feature_eng': [\n",
    "            None,\n",
    "            PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "            PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)\n",
    "        ],\n",
    "        'clf': [LGBMClassifier()],\n",
    "        'clf__n_estimators': stats.randint(1, 100),\n",
    "        'clf__learning_rate': stats.uniform(0.01, 0.7),\n",
    "        'clf__max_depth': [None, stats.randint(1, 6)]\n",
    "    }, {\n",
    "        'data_transformations': [\n",
    "            data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4\n",
    "        ],\n",
    "        'feature_eng': [\n",
    "            None, \n",
    "            PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "            PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)\n",
    "        ],\n",
    "        'clf': [XGBClassifier()],\n",
    "        'clf__n_estimators': stats.randint(5, 125),\n",
    "        'clf__eta': stats.uniform(0.01, 1),\n",
    "        'clf__max_depth': [None, stats.randint(1, 6)],\n",
    "        'clf__gamma': stats.uniform(0.01, 1)\n",
    "    }, {\n",
    "        'data_transformations': [\n",
    "            data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4\n",
    "        ],\n",
    "        'feature_eng': [\n",
    "            None, \n",
    "            PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "            PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)\n",
    "        ],\n",
    "        'clf': [StackingClassifier(estimators=[\n",
    "                ('svc', SVC(C=1, gamma=1)),\n",
    "                ('rf', RandomForestClassifier(max_depth=7, max_features=None, n_estimators=60, n_jobs=-1, random_state=28)),\n",
    "                ('xgb', XGBClassifier(eta=0.6, gamma=0.7, max_depth=None, n_estimators=30))\n",
    "            ], final_estimator=LogisticRegression(C=1)\n",
    "        )]\n",
    "    }, {\n",
    "        'data_transformations': [\n",
    "            data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4\n",
    "        ],\n",
    "        'feature_eng': [\n",
    "            None,\n",
    "            PCA(n_components=round(x_train.shape[1]*0.9)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.8)),\n",
    "            PCA(n_components=round(x_train.shape[1]*0.7)),\n",
    "            PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)\n",
    "        ],\n",
    "        'clf': [VotingClassifier(estimators=[\n",
    "                ('gbt', GradientBoostingClassifier(learning_rate=0.8, max_depth=None, n_estimators=30)),\n",
    "                ('lgbm', LGBMClassifier(n_estimators=30, learning_rate=0.6, max_depth=None)),\n",
    "                ('xgb', XGBClassifier(eta=0.8, gamma=0.8, max_depth=None, n_estimators=40))\n",
    "            ], voting='soft')\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    \n",
    "    # Now, we fit a RandomSearchCV to search over the grid of parameters defined above\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    \n",
    "    best_model_pipeline = RandomizedSearchCV(pipe, params_grid, n_iter=300, \n",
    "                                             scoring=metrics, refit='accuracy', \n",
    "                                             n_jobs=-1, cv=5, random_state=21)\n",
    "\n",
    "    best_model_pipeline.fit(x_train, y_train)\n",
    "    \n",
    "        \n",
    "    # At last, we check the final results\n",
    "    outstr1 = \"\\n\\n#---------------- Best Data Pipeline found in RandomSearchCV  ----------------#\\n\\n\"\n",
    "    print(outstr1, best_model_pipeline.best_estimator_[0])\n",
    "    \n",
    "    outstr2 = \"\\n\\n#---------------- Best Feature Engineering technique found in RandomSearchCV  ----------------#\\n\\n\"\n",
    "    print(outstr2, best_model_pipeline.best_estimator_[1])\n",
    "    \n",
    "    outstr3 = \"\\n\\n#---------------- Best Classifier found in RandomSearchCV  ----------------#\\n\\n\"\n",
    "    print(outstr3, best_model_pipeline.best_estimator_[2])\n",
    "    \n",
    "    outstr4 = \"\\n\\n#---------------- Best Estimator's average Accuracy Score on CV (validation set) ----------------#\\n\\n\"\n",
    "    print(outstr4, best_model_pipeline.best_score_)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, best_model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function above, returing train/test data and best model's pipeline\n",
    "x_train, x_test, y_train, y_test, best_model_pipeline = defineBestModelPipeline(\n",
    "    df, target, categorical_columns, numeric_columns\n",
    ")\n",
    "\n",
    "# Checking best model's performance on test data\n",
    "test_set_results = testSetResultsClassifier(best_model_pipeline, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing all results and metrics, from all models, obtained by the RandomSearchCV steps\n",
    "df_results = pd.DataFrame(best_model_pipeline.cv_results_)\n",
    "\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now visualizing all results and metrics obtained only by the best classifier\n",
    "display(df_results[df_results['rank_test_accuracy'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we access the categorical feature names generated by OneHotEncoder, and then concatenate them\n",
    "# with the numerical feature names, in the same order our pipeline is applying data transformations.\n",
    "categorical_features_after_onehot = best_model_pipeline.best_estimator_.named_steps['data_transformations']\\\n",
    "                                        .transformers_[1][1].named_steps['onehot'].get_feature_names()\n",
    "\n",
    "feature_names_in_order = numeric_columns + categorical_features_after_onehot\n",
    "\n",
    "print(feature_names_in_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting feature importances of the best model, if sklearn tree-based (top 5 features)\n",
    "#print(\"\\n#---------------- Bar plot with feature importances ----------------#\")\n",
    "#feat_importances = pd.Series(best_model_pipeline.best_estimator_.named_steps['clf'].feature_importances_, index=feature_names_in_order)\n",
    "#feat_importances.nlargest(5).plot(kind='barh')\n",
    "\n",
    "\n",
    "# # Plotting feature importances of the best model, if linear regression-based (top 5 features)\n",
    "#print(\"\\n#---------------- Bar plot with feature importances ----------------#\")\n",
    "#feat_importances = pd.Series(best_model_pipeline.best_estimator_.named_steps['clf'].coef_, index=feature_names_in_order)\n",
    "#feat_importances.nlargest(5).plot(kind='barh')\n",
    "\n",
    "\n",
    "# # Plotting feature importances for XGB Model\n",
    "#plot_importance_xgb(best_model_pipeline.best_estimator_.named_steps['clf'], height=0.4, \n",
    "#title='Feature Importances for XGB Classifier', importance_type='gain')\n",
    "\n",
    "\n",
    "# # Plotting feature importances for LGBM Model\n",
    "#plot_importance_lgbm(best_model_pipeline.best_estimator_.named_steps['clf'], \n",
    "#                     figsize=(10, 4), title='Feature importances for LGBM Classifier',\n",
    "#                     importance_type='gain', max_num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
